{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Group - 20 \n",
    "\n",
    "Members: \n",
    "\n",
    "1. Sheikh Arfahmi Bin Sheikh Arzimi (6452868)\n",
    "\n",
    "2. Ewan Brett (6525318)\n",
    "\n",
    "3. Cedric Nissen (6560733)\n",
    "\n",
    "4. Nils Hollnagel (6540848)\n",
    "\n",
    "5. Luka Rehviašvili (6299318)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Objective (Luka)\n",
    "\n",
    "To develop an interactive dashboard that aids effective crowd flow management for the SAIL2025 event in Amsterdam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Effective crowd management is essential to ensure public safety and improve visitors’ experience at large-scale events such as SAIL 2025 in Amsterdam. Without sufficient crowd monitoring, crowd managers may lack oversight of crowd densities, potentially increasing the risk of overcrowding and related safety incidents. Past tragedies, such as the Seoul Halloween Parade in 2022, Houston’s Astroworld Festival in 2021 and Germany’s Love Parade in 2010, underscore the importance of proactive crowd monitoring and prediction systems. This project, therefore, aims to develop an interactive dashboard that serves to aid the SAIL Crowd Monitoring Team (CMT) in making informed decisions in real-time to manage crowd levels effectively and efficiently. Beyond SAIL, this dashboard could also be a modular tool which can be implemented by other large-scale event organisers worldwide. This project is also part of the broader crowd management strategy and will be integrated with physical control measures to form a holistic solution to crowd management challenges. \n",
    "\n",
    "To be adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Questions\n",
    "\n",
    "RO: To develop an interactive dashboard that aids effective crowd flow management for the SAIL2025 event in Amsterdam.​\n",
    "​\n",
    "RQ1: Which measure can be used to enable secure access control?​\n",
    "RQ2: How can crowd flow be visualised most effectively?​\n",
    "RQ3: Which algorithm is suitable to predict crowd flow based on the available datasets?\n",
    "\n",
    "To be expanded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Used (Luka)\n",
    "\n",
    "To be adjusted\n",
    "\n",
    "## Confirmed Datasets\n",
    "\n",
    "1.\tCrowd flow (based on sensor counts)\n",
    "\n",
    "    •\tNumber of sensors: 46 locations, bidirectional\n",
    "\n",
    "    •\tRefresh rate: Every 3 minutes\n",
    "\n",
    "2.\tSAIL Event timetable (https://www.sail.nl/programma-en-plattegrond)\n",
    "\n",
    "3.\tGeospatial Information of Sail 2025 Area (OpenStreetMaps, ArcGIS)\n",
    "\n",
    "## Potential Datasets (Pending Requests)\n",
    "\n",
    "1.\tVessel position\n",
    "\n",
    "2.\tPositions of traffic marshals\n",
    "\n",
    "3.\tNS Train (live) Timetables (https://ndovloket.nl/index.html) \n",
    "\n",
    "4.\tGVB (live) Timetables - Metros, Trams, Buses (https://ndovloket.nl/index.html) \n",
    "\n",
    "5.\tMeteorological data (based on KNMI data, alternative https://www.wunderground.com/history/weekly/nl/schiphol/EHAM/date/2025-8-20 copy to Excel and export as CSV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering RQ1: User Authentication (Nils)\n",
    "\n",
    "RQ1: Which measure can be used to enable secure access control?​\n",
    "\n",
    "To be added\n",
    "\n",
    "Explain User Authentication feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering RQ2: Crowd Flow Visualisation (Ewan, Cedric)\n",
    "\n",
    "To be adjusted --> Focus on methodology and results\n",
    "\n",
    "- Data Pipeline (Ingestion, Processing, Storage, Analysis, Visualisation) for current crowd flow\n",
    "- features (Auto-refresh, user settings, color-coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Pipelin (Based on research proposal)\n",
    "\n",
    "1.\tData Ingestion \n",
    "\n",
    "    •\tUsage of confirmed datasets listed above. \n",
    "\n",
    "    •\tIn case access is granted, the above listed “potential datasets” will also be considered.\n",
    "\n",
    "2.\tData Processing/Transformation \n",
    "\n",
    "    •\tCompile all the data into a time-series pandas DataFrames, for easy reading, updating and plotting.\n",
    "\n",
    "    •\tClean and standardise data to ensure consistency across sources.\n",
    "\n",
    "3.\tData Storage \n",
    "\n",
    "    •\tStore all data on shared drives that are backed up on university servers – OneDrive.\n",
    "\n",
    "    •\tHave a redundant version of the data, in case a file becomes corrupted and unusable. \n",
    "\n",
    "4.\tData Analysis & Prediction \n",
    "\n",
    "    •\tAnalysis of the retrieved data \n",
    "\n",
    "            i.\tCleaning\n",
    "\n",
    "            ii.\tRemoving the duplicates \n",
    "\n",
    "            iii. Making sure there are no inconsistencies\n",
    "\n",
    "            iv.\tBias checks\n",
    "\n",
    "                1.\tInspect the representation across key groups \n",
    "\n",
    "                2.\tCheck missingness, errors and label quality by group\n",
    "\n",
    "    •\tThe usage of the appropriate data analysis tool (Python and Excel (in case of))\n",
    "\n",
    "    •\tPrediction will be based on the need and the initial results\n",
    "\n",
    "5.\tData Visualisation & Delivery\n",
    "\n",
    "    •\tDeliver dashboard as a Web Application using streamlit\n",
    "\n",
    "    •\tDisplay interactive maps, charts and tables based on the preference of the CMT, adapting the view based on size and category of the dataset respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering RQ3: Crowd Flow Predictions (Sheikh)\n",
    "\n",
    "To be added\n",
    "\n",
    "- Methodology\n",
    "- Results\n",
    "- Crital Evaluation (Accuracy of Prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3 Crowd Count Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Introduction \n",
    "\n",
    "Crowd count prediction serves to estimate the number of people across space and time. In this case, space is divided based on the location of crowd count sensors and time is split into 3-minute intervals based on the sensor update frequency. Therefore, the goal is to predict the future crowd counts for each available sensor in multiples of 3-minutes intervals (i.e 3, 6, 9 ... into the future). The Extreme Gradient Boosting (XGBoost) machine learning model was experimented with in this project. It was selected due to its robustness and ability to predict non-linear relationships. The next few subsections will outline the steps taken to develop this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Data Sources / Requirements / Preprocessing\n",
    "\n",
    "The data sources used are: (i) Crowd Count from Sensors (by timestamp and sensor name), and (ii) KNMI Weather Data (by timestamp, includes Temperature, Dew Point, Air Pressure, Windspeed, Max Gust, Rainfall, Sunshine Duration). Both are comma-separated-value (.csv) files located in [sensor_data](data/sensor_data.csv) and [weather_data](data/weather_data.csv) respectively. They were imported, inspected for missing values and cleaned. The data was then visualised to capture general trend(s). This process can be found in [Read_Sensor_(Crowd)_Data](Notebooks/Read_KNMI_WeatherData.ipynb) and [Read_KNMI_WeatherData](Notebooks/Read_KNMI_WeatherData.ipynb). Both datasets were then merged into one single dataframe with timestamp set as index and saved as [crowd_weather_merged.csv](Notebooks/data/crowd_weather_merged.csv) . The crowd count and weather trends are shown below.\n",
    "\n",
    "![Crowd Count Trend](screenshots/sensor_trend.png)\n",
    "\n",
    "From the almost sinuisoidal curve of the crowd count, we can tell that the overall crowd count peaks daily at around late afternoon and dips slightly after midnight. This is the general crowd trend, but the distribution across sensors may vary greatly. \n",
    "\n",
    "![Weather Trend](screenshots/weather_trend.png)\n",
    "\n",
    "From the weather data, no general trends can be concluded. However, rainfall was zero for most of the event and thus, it can be hypothesised that rainfall will not be a strong feature to predict the crowd count. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Target and Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target 'Y' of the XBG model is the data to be predicted, which is the future crowd count of all sensors. The features, 'X', are the remaining data in [crowd_weather_merged.csv](data/crowd_weather_merged.csv). They include 'hour', 'minute', 'day', 'month', 'weekday', 'is_weekend', 'temperature' , 'dew_point', 'air_pressure', 'wind_speed', 'max_gust', 'rainfall', 'sunshine_duration' and 'relative_humidity'.\n",
    "\n",
    "Prior to training the XGB model, pre-ML data exploration was conducted to investigate the correlation between the different data types. This process can be followed in [Pre_ML_DataExploration](Notebooks/Pre_ML_DataExploration.ipynb). The data exploration gives us insights on the expected results and feature importances of the trained machine learning model. \n",
    "\n",
    "![Correlation Matrix: Sensor vs Sensor](screenshots/corr_sensor_vs_sensor.png)\n",
    "\n",
    "For example, in the correlation matrix among crowd count of sensors above, we can see a positive correlation among sensors pairs with  opposing directions (CMSA-GAKH-01_0 and CMSA-GAKH-01_180). However, there is weak correlation between sensors that are further apart. (Note that in the figures below, there is insufficient space to print all the y labels)\n",
    "\n",
    "![Correlation Matrix: Sensor vs Weather](screenshots/corr_sensor_vs_weather.png)\n",
    "\n",
    "The correlation matrix between weather data and crowd data was also plotted to see which feature has strong or weak correlation with crowd data. As shown in the figure above, the 'hour' feature has the strongest overall correlation with the crowd count. It is thus expected to be one of the most important feature for the XGB model. Other potentially important features include 'temperature', 'wind speed', 'max_gust', 'sunshine duration' and 'relative humidity'. \n",
    "\n",
    "From the graph of Total Crowd Count over Time above, it was found that historical data may be able to predict current data as the crowd count shows some seasonality characteristics. To determine how far back in time should crowd count be considered as feature, an Autocorrelation Function (ACF) graph of CMSA-GAKH-01_0 was plotted. The x-axis of the graph indicates the time interval between the current observation and past ones, also know as the 'lag', while the y-axis represents the correlation values between current observation and observation at a particular lag. The blue shaded area represents the 95% confidence interval. Any bars that extend beyond (above/below) the blue area is statistically significant. \n",
    "\n",
    "![Autocorrelation of CMSA-GAKH-01_0](screenshots/autocorr_CMSA_01_0_1000lags.png)\n",
    "\n",
    "From the graph above, it was observed that there are significant spikes at regular intervals of 200-250 steps. This confirms seasonality in the crowd data, yet not all of the lags are statistically significant. Therefore, another ACF was plotted to zoom into the lags that are statistically significant.\n",
    "\n",
    "![Autocorrelation of CMSA-GAKH-01_0](screenshots/autocorr_CMSA_01_0.png)\n",
    "\n",
    "From the graph above, we can deduce that lags up to lag_75 are statistically significant and thus will be considered as features. \n",
    "\n",
    "In addition to lag features, rolling mean features were also included. Rolling mean is the average of crowd count over a fixed interval of historic data. It's purpose is to smooth out short-term volatility and reduce noise to reveal underlying and longer term trends. The rolling mean intervals were chosen to capture short-term (3-5), medium-term (10-20) and long-term (30-60) trends. The additional features were created and then appended to the merged dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Training Procedure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train-Test Split](screenshots/train_test_split.png)\n",
    "\n",
    "The features and target were split chronologically with data from 20-23 August 2025 assigned as training data, while the remaining data (23 August 2025, 00:00 onwards) assigned as test data. Random splitting were also experimented with in [TS_2_RandomSplit_MultiOutputReg](Archived_PastNotebooks/TS_2_RandomSplit_MultiOutputReg.ipynb). However, the approach involves the leakage of future data into the prediction of past/current data and was thus rejected as a realistic approach. \n",
    "\n",
    "Another approach in the training strategy was also to train one model per sensor ([TS_1_RandomSplit](Archived_PastNotebooks/TS_1_RandomSplit.ipynb)), as opposed to one universal model for all sensors. However, upon quick inspection of the crowd count of all the sensors, it was observed that most sensors follow the seasonality pattern and peak at similar times. Thus, a single model was preferred for ease of maintenance and to reduce overfitting. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Model Evaluation\n",
    "\n",
    "The metrics used to evaluate the model are mean absolute error (MAE) and root-mean-squared error (RMSE). Instead of focusing on the performance of the model in general, the model was evaluated based on how well it predicts data for each location. This is more meaningful as it accounts for the differences between locations. The model performed best for GASA-06_95 with MAE = 0.080795 and RMSE = 0.087036.\n",
    "\n",
    "![Actual vs Predicted: GASA-06_95](screenshots/GASA-06_95.png)\n",
    "\n",
    "On the other hand, it performed worst for GASA-02-02_135 with MAE = 25.207134 and RMSE = 58.591652. \n",
    "\n",
    "![Actual vs Predicted: GASA-02-02_135](screenshots/GASA-02-02_135.png)\n",
    "\n",
    "The model's feature importance was validated against insights gained from data exploratory. \n",
    "\n",
    "![Feature Importance](screenshots/feature_importance.png)\n",
    "\n",
    "Based on the figure above, the mean of the past 3 time intervals (past 9 minutes) was the most important feature in predicting the current crowd count. The top features were also dominated by rolling mean and lag features, which matches the strong seasonality pattern observed across most of the locations. In general, weather features were less important compared to historical data, though it should be noted that temperature is the most important feature in the model among all weather features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Deployment Strategy \n",
    "\n",
    "The XGB model is stored locally as [crowd_count_model.pkl](Notebooks/crowd_count_model.pkl) and loaded in [4_Predictive_Analysis.py](pages/4_Predictive_Analysis.py). The streamlit app fetches live sensor data via load_live_sensor_data() in [data_loader.py](data_loader.py) every 3 minutes. A function create_features() was created to transform historic and current data as inputs to the model. Predictions are genererated on demand per sensor selection. By default, the forecasting window is set to 20 intervals (1 hour) and the prediction is done recursively using recursive_forecast() function. The results were plotted in an interactive line graph that shows historical, current and predicted counts. For the purpose of visualising the accuracy of predictions, actual future counts were also plotted. In terms of User Interface, colour scheme was standardised across all graphs for the different type of data, with legends present to assist users. The default zoom focuses on the past hour plus the forecast horizon for immediate insight. The figure below shows a screenshot of the predictive analytics visualisation. \n",
    "\n",
    "![PredictiveAnalytics](screenshots/predict_graph.png)\n",
    "\n",
    "In terms of extensibility, The model can be updated periodically with new sensor and weather data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Limitations of Model \n",
    "\n",
    "The [weather data from KNMI](Notebooks/data/uurgeg_240_2021-2030.txt) was tracked hourly. Therefore, shorter-term changes in weather conditions were not captured and this could explain the weak importance of weather features in predicting the crowd count. Additionally, more data, such as event timeline and location, vessel position, carflow data, could be added into the pipeline and this could strengthen the model. In terms of training strategy, the trained XGB model was tested only on weekend data. This explains why the 'isWeekend' feature is not important at all. If the SAIL event was longer with more data points, the train-test split could be done such that the training and testing data consist of both weekday and weekend data while still maintaining a chronological split. The hyperparameters of the XGB model could also be optimised with Hyperparameter Tuning, creating a model with the best performance for this use case. However, this was not done due to time limitation of this project. Lastly, to increase usability and user-friendliness, the predicted values for all sensors could be visualised in a map with adjustable time horizon. Thresholds could be set to warn crowd monitoring team of higher than acceptable crowd counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion (Nils)\n",
    "\n",
    "To be added\n",
    "\n",
    "- Main Research Findings\n",
    "- Limitations\n",
    "- Outlook --> Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribution Statement (Everyone)\n",
    "\n",
    "*Be specific. Some of the tasks can be coding (expect everyone to do this), background research, conceptualisation, visualisation, data analysis, data modelling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author 1: Sheikh **: \n",
    "- Future Crowd Flow Prediction (ML Module)\n",
    "- Project Management (Task Distribution)\n",
    "\n",
    "**Author 2: Nils **: \n",
    "- User Authentication & Security\n",
    "- Project Management (Report Structure, Task Distribution, Group Organisation)\n",
    "\n",
    "**Author 3: Cedric **: \n",
    "- Reports, Settings & Multi-User Management\n",
    "\n",
    "**Author 4: Ewan **: \n",
    "- Visualization / Dashboard Interface\n",
    "\n",
    "**Author 5: Luka **: \n",
    "- Current Crowd Flow Data Pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TIL6022-25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
